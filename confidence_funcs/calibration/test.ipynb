{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf:\n",
    "    input_dimension: 10\n",
    "    output_dimension: 10 \n",
    "    layers : [  ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf = {}\n",
    "model_conf['input_dimension'] = 2\n",
    "model_conf['output_dimension'] = 2 \n",
    "\n",
    "model_conf['layers'] = [\n",
    "    { \"type\":\"linear\", \"dim_factor\":2},\n",
    "    { \"type\": \"activation\", \"act_fun\": \"relu\"},\n",
    "    {  \"type\":\"linear\", \"dim_factor\":2},\n",
    "    { \"type\": \"activation\", \"act_fun\": \"relu\"},\n",
    "]\n",
    "model_conf['layers'] = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from torch import optim\n",
    "import torch.nn.functional as F  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, model_conf):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.in_dim  = model_conf['input_dimension']\n",
    "        self.out_dim = model_conf['output_dimension']\n",
    "        #k       = model_conf['num_classes']\n",
    "        layers = nn.ModuleList()\n",
    "\n",
    "        if('layers' not in model_conf or model_conf['layers'] is None or len(model_conf['layers'])==0):\n",
    "            layers.add_module('layer_0', nn.Linear(self.in_dim,self.out_dim))\n",
    "\n",
    "        elif( 'layers' in model_conf and  len(model_conf['layers']) > 0):\n",
    "            p_dim = [self.in_dim]\n",
    "\n",
    "            for i in range(len(model_conf['layers'])):\n",
    "                l_conf = model_conf['layers'][i]\n",
    "                layer = None                 \n",
    "                if(l_conf['type']=='linear'):\n",
    "                    d2 = int(self.in_dim * l_conf['dim_factor'])\n",
    "                    layer =  nn.Linear(  p_dim[i], d2)\n",
    "                    p_dim.append(d2)\n",
    "\n",
    "                elif(l_conf['type'] == 'activation'):\n",
    "                    if(l_conf['act_fun']=='relu'):\n",
    "                        layer =  nn.ReLU() \n",
    "                    elif(l_conf['act_fun']=='tanh'):\n",
    "                        layer = nn.Tanh()\n",
    "                    p_dim.append(p_dim[i])\n",
    "\n",
    "                layers.add_module( f'layer_{i}' , layer)\n",
    "            \n",
    "            layers.add_module('last_layer', nn.Linear( p_dim[-1], self.out_dim) )\n",
    "\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x ):\n",
    "        z = self.net(x)\n",
    "        return z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP(model_conf)\n",
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0,0]).float() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1347,  0.2337], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([1., 1.])\n",
      "tensor([1., 1.])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.data) # = nn.parameter.Parameter(torch.ones_like(param))\n",
    "    param.data =  nn.parameter.Parameter(torch.ones_like(param))\n",
    "    print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3000, 0.3000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul( list(model.parameters())[0].data,x  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "act-learn-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
